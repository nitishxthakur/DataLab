{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN23SUfDITUdOKDrnprwTsG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nitishxthakur/DataLab/blob/main/data_cleaning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "fMB2-dxLwN3s"
      },
      "outputs": [],
      "source": [
        "text=\"Let's take your sentence and apply all the steps of text cleaning, including extra elements like emojis, URLs, extra spaces, and hashtags.\\nHello Everyone!!!! ðŸ˜ŠðŸ˜Š How are you doing? are you excited for today's lecture? Let me introduce myself.... I am Sushrutha.. .. This is my GitHub Profile.... https://www.sushrutha.linkedin.com... I have 8 years experience in the Data Science Domain... 2.5 years of corporate exp and 5.5 years od research experience.. I am here to teach you your NLP course.... #ThirdClass #NLP #LLM\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text=text.lower()"
      ],
      "metadata": {
        "id": "nyyX2VyVwOqE"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "text=text.translate(str.maketrans(\"\", \"\", string.punctuation))"
      ],
      "metadata": {
        "id": "xbh83l_PwVlU"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "text=re.sub(r'<.*?>', '', text)"
      ],
      "metadata": {
        "id": "HduhQHy9whXS"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text=re.sub(r'http\\S+|www.\\S+', '', text)"
      ],
      "metadata": {
        "id": "bqPmSoCXwtQ4"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text=re.sub(r'#\\w+', '', text)"
      ],
      "metadata": {
        "id": "IqP9F-YdxBL5"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text=re.sub(r'[^\\w\\s]', '', text)"
      ],
      "metadata": {
        "id": "VMyZylenxPyY"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text=re.sub(r'\\d+', '', text)"
      ],
      "metadata": {
        "id": "FMIhHjwMyoHh"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "NayxniTFyu3s",
        "outputId": "69a58853-9322-4c3e-a25b-d2a650a60c9b"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'lets take your sentence and apply all the steps of text cleaning including extra elements like emojis urls extra spaces and hashtags\\nhello everyone  how are you doing are you excited for todays lecture let me introduce myself i am sushrutha  this is my github profile  i have  years experience in the data science domain  years of corporate exp and  years od research experience i am here to teach you your nlp course thirdclass nlp llm'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt') # downloads the punkt resource\n",
        "\n",
        "# Convert the list of strings back to a single string\n",
        "text = \" \".join(text)\n",
        "\n",
        "text = word_tokenize(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mgAimlspz-uu",
        "outputId": "4d96ec54-0293-4ac3-b278-68dfd5095834"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "# Step 5: Stemming\n",
        "def apply_stemming(tokens):\n",
        "    stemmer = PorterStemmer()\n",
        "    return [stemmer.stem(word) for word in tokens]\n",
        "\n",
        "# Apply Stemming\n",
        "tokens_stemmed = apply_stemming(text)\n",
        "print(tokens_stemmed)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YyHoZgu70ogF",
        "outputId": "6408fd0f-6f96-43ca-d771-293bb2c2985e"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['l', 'e', 't', 's', 't', 'a', 'k', 'e', 'y', 'o', 'u', 'r', 's', 'e', 'n', 't', 'e', 'n', 'c', 'e', 'a', 'n', 'd', 'a', 'p', 'p', 'l', 'y', 'a', 'l', 'l', 't', 'h', 'e', 's', 't', 'e', 'p', 's', 'o', 'f', 't', 'e', 'x', 't', 'c', 'l', 'e', 'a', 'n', 'i', 'n', 'g', 'i', 'n', 'c', 'l', 'u', 'd', 'i', 'n', 'g', 'e', 'x', 't', 'r', 'a', 'e', 'l', 'e', 'm', 'e', 'n', 't', 's', 'l', 'i', 'k', 'e', 'e', 'm', 'o', 'j', 'i', 's', 'u', 'r', 'l', 's', 'e', 'x', 't', 'r', 'a', 's', 'p', 'a', 'c', 'e', 's', 'a', 'n', 'd', 'h', 'a', 's', 'h', 't', 'a', 'g', 's', 'h', 'e', 'l', 'l', 'o', 'e', 'v', 'e', 'r', 'y', 'o', 'n', 'e', 'h', 'o', 'w', 'a', 'r', 'e', 'y', 'o', 'u', 'd', 'o', 'i', 'n', 'g', 'a', 'r', 'e', 'y', 'o', 'u', 'e', 'x', 'c', 'i', 't', 'e', 'd', 'f', 'o', 'r', 't', 'o', 'd', 'a', 'y', 's', 'l', 'e', 'c', 't', 'u', 'r', 'e', 'l', 'e', 't', 'm', 'e', 'i', 'n', 't', 'r', 'o', 'd', 'u', 'c', 'e', 'm', 'y', 's', 'e', 'l', 'f', 'i', 'a', 'm', 's', 'u', 's', 'h', 'r', 'u', 't', 'h', 'a', 't', 'h', 'i', 's', 'i', 's', 'm', 'y', 'g', 'i', 't', 'h', 'u', 'b', 'p', 'r', 'o', 'f', 'i', 'l', 'e', 'i', 'h', 'a', 'v', 'e', 'y', 'e', 'a', 'r', 's', 'e', 'x', 'p', 'e', 'r', 'i', 'e', 'n', 'c', 'e', 'i', 'n', 't', 'h', 'e', 'd', 'a', 't', 'a', 's', 'c', 'i', 'e', 'n', 'c', 'e', 'd', 'o', 'm', 'a', 'i', 'n', 'y', 'e', 'a', 'r', 's', 'o', 'f', 'c', 'o', 'r', 'p', 'o', 'r', 'a', 't', 'e', 'e', 'x', 'p', 'a', 'n', 'd', 'y', 'e', 'a', 'r', 's', 'o', 'd', 'r', 'e', 's', 'e', 'a', 'r', 'c', 'h', 'e', 'x', 'p', 'e', 'r', 'i', 'e', 'n', 'c', 'e', 'i', 'a', 'm', 'h', 'e', 'r', 'e', 't', 'o', 't', 'e', 'a', 'c', 'h', 'y', 'o', 'u', 'y', 'o', 'u', 'r', 'n', 'l', 'p', 'c', 'o', 'u', 'r', 's', 'e', 't', 'h', 'i', 'r', 'd', 'c', 'l', 'a', 's', 's', 'n', 'l', 'p', 'l', 'l', 'm']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NRBD9-ji2QoY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}